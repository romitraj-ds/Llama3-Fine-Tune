{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjz7X63gzB_V"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDCMD7qmzB_W"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "qtlH3TtrWBHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "import torch\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "k7bP3NJ4VpRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xBxlzhezB_W"
      },
      "source": [
        "## Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "original_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "## data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvOPfPnet76H"
      },
      "outputs": [],
      "source": [
        "huggingface_dataset_name = \"neil-code/dialogsum-test\"\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'].column_names"
      ],
      "metadata": {
        "id": "3-a5n36qAo7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    \"\"\"\n",
        "    Format various fields of the sample ('instruction','output')\n",
        "    Then concatenate them using two newline characters\n",
        "    :param sample: Sample dictionnary\n",
        "    \"\"\"\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n",
        "    RESPONSE_KEY = \"### Output:\"\n",
        "    END_KEY = \"### End\"\n",
        "\n",
        "    blurb = f\"\\n{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\"\n",
        "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
        "    end = f\"{END_KEY}\"\n",
        "\n",
        "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
        "\n",
        "    formatted_prompt = \"\\n\\n\".join(parts)\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "7qb5KnAwAWRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Tokenizing a batch\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "QJFRu94AE_RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def preprocess_dataset(tokenizer, max_length: int,seed, dataset):\n",
        "    \"\"\"Format & tokenize it so it is ready for training\n",
        "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(create_prompt_formats)\n",
        "\n",
        "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True,\n",
        "        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n",
        "    )\n",
        "\n",
        "    # Filter out samples that have input_ids exceeding max_length\n",
        "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    dataset = dataset.shuffle(seed=seed)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "hWIc5wT_FDXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Pre-process dataset\n",
        "max_length = get_max_length(original_model)\n",
        "print(max_length)\n",
        "seed = 4012\n",
        "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n",
        "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])"
      ],
      "metadata": {
        "id": "k6MAFGBVFD2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tune Model\n"
      ],
      "metadata": {
        "id": "Fjxe98hPEd5-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "apply LoRA adapters to a pre-trained language model. This allows for efficient\n",
        "fine-tuning by only updating a small percentage of the model's parameters.\n",
        "\n",
        "attr->\n",
        "model: The pre-trained language model to be finetuned.\n",
        "r: This is the LoRA rank. It determines the dimensionality of the\n",
        "   low-rank matrices used in the adapters.\n",
        "   higher rank -> more params trained -> better perf -> more vram\n",
        "traget_modules: module name where LoRA adapters will be applied\n",
        "lora_alpha: This is the scaling factor for the LoRA adapters\n",
        "lora_dropout: droput applied to LoRA layers\n",
        "bias: This specifies whether to train the bias parameters in the LoRA layers\n",
        "use_gradient_checkpointing: enables gradient checkpointing, which can reduce\n",
        "memory usage during training by recomputing gradients instead of storing them.\n",
        "\"\"\"\n",
        "# r = Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "\n",
        "patched_model = FastLanguageModel.get_peft_model(\n",
        "    original_model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = patched_model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # max_steps = 60,\n",
        "        num_train_epochs = 1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        gradient_checkpointing=True,\n",
        "        eval_steps=1,\n",
        "        do_eval=True,\n",
        "        report_to = \"none\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Inference"
      ],
      "metadata": {
        "id": "zklFuT6yEvkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(model, prompt, max_length=1000):\n",
        "    \"\"\"\n",
        "    Generates text from a given model and prompt.\n",
        "\n",
        "    Args:\n",
        "        model: The language model to use for generation.\n",
        "        prompt (str): The input prompt.\n",
        "        max_length (int): The maximum number of tokens to generate.\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing the generated text.\n",
        "    \"\"\"\n",
        "    # Encode the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = inputs.to(model.device)\n",
        "\n",
        "    # Generate text\n",
        "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "I5dRFHavBOvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output(model, dataset, index):\n",
        "    prompt = dataset[index]['dialogue']\n",
        "    summary = dataset[index]['summary']\n",
        "\n",
        "    formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
        "    res = gen(model,formatted_prompt,1000)\n",
        "\n",
        "    output = res[0].split('Output:\\n')[1]\n",
        "\n",
        "    dash_line = '-'.join('' for x in range(100))\n",
        "    print(dash_line)\n",
        "    print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ],
      "metadata": {
        "id": "Xyk0GcKSBS9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_output(original_model, dataset['test'], 1)"
      ],
      "metadata": {
        "id": "BEZqmolxOr8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(patched_model)\n",
        "get_output(patched_model, dataset['test'], 1)"
      ],
      "metadata": {
        "id": "G77da3WxPQZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "patched_model.save_pretrained(\"llama3_8B_summarise_fine_tune\")\n",
        "tokenizer.save_pretrained(\"llama3_8B_summarise_fine_tune\")\n",
        "patched_model.push_to_hub(\"romitraj-ds/llama3_8B_summarise_fine_tune\", token = \"TOKEN\")\n",
        "tokenizer.push_to_hub(\"romitraj-ds/llama3_8B_summarise_fine_tune\", token = \"TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n"
      ],
      "metadata": {
        "id": "nP_VtW8qRXJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "    human_baseline_text_output = human_baseline_summaries[idx]\n",
        "    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
        "\n",
        "    original_model_res = gen(original_model,prompt,400)\n",
        "    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n",
        "\n",
        "    peft_model_res = gen(patched_model,prompt,400)\n",
        "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
        "    peft_model_text_output, success, result = peft_model_output.partition('###')\n",
        "\n",
        "    original_model_summaries.append(original_model_text_output)\n",
        "    peft_model_summaries.append(peft_model_text_output)\n",
        "    print(f\"Done: {idx+1}\")\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
        "df"
      ],
      "metadata": {
        "id": "KmqtGyUmRaBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)\n",
        "\n",
        "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
        "\n",
        "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "for key, value in zip(peft_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ],
      "metadata": {
        "id": "R0TzmtmxRa2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result\n",
        "\n",
        "**ORIGINAL MODEL:**\n",
        "\n",
        "{'rouge1': np.float64(0.31705290845026823), 'rouge2': np.float64(0.10047710327274166), 'rougeL': np.float64(0.26201278643250203), 'rougeLsum': np.float64(0.24806785489138694)}\n",
        "\n",
        "**PEFT MODEL:**\n",
        "\n",
        "{'rouge1': np.float64(0.42695474575617476), 'rouge2': np.float64(0.16893660011240563), 'rougeL': np.float64(0.3473905790841749), 'rougeLsum': np.float64(0.34841494823852615)}\n",
        "\n",
        "**Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL**\n",
        "\n",
        "rouge1: 10.99%\n",
        "rouge2: 6.85%\n",
        "rougeL: 8.54%\n",
        "rougeLsum: 10.03%"
      ],
      "metadata": {
        "id": "X0wF4nQEn54y"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mjz7X63gzB_V",
        "4xBxlzhezB_W",
        "vITh0KVJ10qX",
        "Fjxe98hPEd5-",
        "zklFuT6yEvkk"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}